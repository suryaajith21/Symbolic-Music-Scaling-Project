{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IAgVMQUfa2Ee",
    "outputId": "46c6d2b3-284d-4124-ecf0-07b63f985945"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: music21 in /usr/local/lib/python3.12/dist-packages (9.9.1)\n",
      "Requirement already satisfied: chardet in /usr/local/lib/python3.12/dist-packages (from music21) (5.2.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from music21) (1.5.2)\n",
      "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.12/dist-packages (from music21) (4.1.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from music21) (3.10.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from music21) (10.8.0)\n",
      "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.12/dist-packages (from music21) (2.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from music21) (2.32.4)\n",
      "Requirement already satisfied: webcolors>=1.5 in /usr/local/lib/python3.12/dist-packages (from music21) (25.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (2.9.0.post0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->music21) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->music21) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->music21) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->music21) (2025.11.12)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->music21) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install music21"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "drive.mount('/content/drive')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gHftpohPdevL",
    "outputId": "4b71277e-7956-418b-8e6e-970e40b1ce99"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path(\"/content/drive/MyDrive/MusicScalingProject\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data/v3\"\n",
    "CHECKPOINT_PATH = PROJECT_ROOT / \"data/checkpoints/BEST_Model.pt\"\n",
    "TOKENIZER_PATH = DATA_DIR / \"tokenizer_bpe_4096.json\"\n",
    "TEST_BIN_PATH = DATA_DIR / \"test.bin\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BestConfig:\n",
    "    n_layer: int = 16       \n",
    "    n_head: int = 16\n",
    "    n_embd: int = 1024\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 4096  \n",
    "    dropout: float = 0.1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.c_proj(y)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None: torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_perplexity(model, data_path, device, block_size):\n",
    "    if not data_path.exists():\n",
    "        print(f\"ERROR: Test file not found at {data_path}\")\n",
    "        return float('inf')\n",
    "\n",
    "    print(f\"Loading test data from {data_path}...\")\n",
    "    data = np.memmap(data_path, dtype=np.uint16, mode='r')\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    batch_size = 4  \n",
    "\n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        iterations = range(0, len(data) - block_size, batch_size * block_size)\n",
    "\n",
    "        for i in tqdm(iterations, desc=\"Calculating Perplexity\"):\n",
    "            chunk_len = min(batch_size * block_size, len(data) - i - 1)\n",
    "            if chunk_len < block_size: break\n",
    "\n",
    "            chunk = torch.from_numpy(data[i:i + chunk_len + 1].astype(np.int64)).to(device)\n",
    "            eff_batch = chunk_len // block_size\n",
    "            if eff_batch == 0: continue\n",
    "\n",
    "            x = chunk[:eff_batch * block_size].view(eff_batch, block_size)\n",
    "            y = chunk[1:eff_batch * block_size + 1].view(eff_batch, block_size)\n",
    "\n",
    "            logits, _ = model(x)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "            total_loss += loss.item() * (eff_batch * block_size)\n",
    "            total_tokens += (eff_batch * block_size)\n",
    "\n",
    "    return math.exp(total_loss / total_tokens) if total_tokens > 0 else float('inf')\n",
    "\n",
    "def evaluate_validity(model, tokenizer_path, device, num_samples=500):\n",
    "    from tokenizers import Tokenizer\n",
    "    import music21\n",
    "\n",
    "    if not tokenizer_path.exists():\n",
    "        print(\"ERROR: Tokenizer not found.\")\n",
    "        return 0, 0\n",
    "\n",
    "    tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    print(f\"\\nGenerating {num_samples} samples for validity check...\")\n",
    "    model.eval()\n",
    "\n",
    "    \n",
    "    start_id = tokenizer.token_to_id(\"<|endoftext|>\")\n",
    "    if start_id is None: start_id = 0\n",
    "\n",
    "    start_tokens = torch.full((num_samples, 1), start_id, dtype=torch.long, device=device)\n",
    "\n",
    "    generated = model.generate(start_tokens, max_new_tokens=256)\n",
    "    valid_syntax = 0\n",
    "    valid_midi = 0\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        text = tokenizer.decode(generated[i].tolist())\n",
    "        \n",
    "        if \"X:\" in text:\n",
    "            text = text[text.find(\"X:\"):]\n",
    "            if \"\\n\\n\" in text: text = text.split(\"\\n\\n\")[0]\n",
    "\n",
    "        try:\n",
    "            s = music21.converter.parse(text, format='abc')\n",
    "            valid_syntax += 1\n",
    "            _ = s.write('midi') \n",
    "            valid_midi += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return (valid_syntax/num_samples)*100, (valid_midi/num_samples)*100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"music21\", \"tokenizers\"])\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    \n",
    "    config = BestConfig()\n",
    "    model = GPT(config)\n",
    "\n",
    "    \n",
    "    if CHECKPOINT_PATH.exists():\n",
    "        print(f\"Loading checkpoint: {CHECKPOINT_PATH}\")\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "        state_dict = checkpoint['model'] if 'model' in checkpoint else checkpoint\n",
    "\n",
    "        \n",
    "        unwanted_prefix = '_orig_mod.'\n",
    "        for k in list(state_dict.keys()):\n",
    "            if k.startswith(unwanted_prefix):\n",
    "                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "        model.load_state_dict(state_dict)\n",
    "    else:\n",
    "        print(f\"CRITICAL: Checkpoint not found at {CHECKPOINT_PATH}\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    \n",
    "    syn, midi = evaluate_validity(model, TOKENIZER_PATH, device)\n",
    "    ppx = calculate_perplexity(model, TEST_BIN_PATH, device, config.block_size)\n",
    "\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"FINAL QUANTITATIVE METRICS\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Test Perplexity:      {ppx:.4f}\")\n",
    "    print(f\"Syntactic Validity:   {syn:.2f}%\")\n",
    "    print(f\"MIDI Conversion:      {midi:.2f}%\")\n",
    "    print(\"=\"*40)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fmjCtApqdQq8",
    "outputId": "4a99752a-6b90-4112-de67-3009ea4c60c5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n",
      "Loading checkpoint: /content/drive/MyDrive/MusicScalingProject/data/checkpoints/BEST_Model.pt\n",
      "\n",
      "Generating 500 samples for validity check...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "abcFormat: WARNING: Could not get pitch information from note:  ^2, assuming C\n",
      "abcFormat: WARNING: Could not get pitch information from note:  ^, assuming C\n",
      "abcFormat: WARNING: Could not get pitch information from note:  _, assuming C\n",
      "abcFormat: WARNING: Could not get pitch information from note:  p, assuming C\n",
      "abcFormat: WARNING: Could not get pitch information from note:  ^, assuming C\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading test data from /content/drive/MyDrive/MusicScalingProject/data/v3/test.bin...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Calculating Perplexity: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 336/336 [00:48<00:00,  6.90it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "========================================\n",
      "FINAL QUANTITATIVE METRICS\n",
      "========================================\n",
      "Test Perplexity:      1.9336\n",
      "Syntactic Validity:   68.00%\n",
      "MIDI Conversion:      58.80%\n",
      "========================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ]
  }
 ]
}